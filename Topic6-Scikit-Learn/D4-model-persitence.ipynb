{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074f02ec",
   "metadata": {},
   "source": [
    "> [!NOTE]  \n",
    "> Once you train the model, it is desirable that the model should be persist for future use so that we do not need to retrain it again and again. It can be done with the help of dump and load features of joblib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib #ERR ImportError: cannot import name 'joblib' from 'sklearn.externals'\n",
    "\n",
    "#INFO in new version it's directly\n",
    "import joblib # It helps you save and load your models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d0114",
   "metadata": {},
   "source": [
    "> I'll go back to yesterday's file to dump and load the model here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d097a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"iris_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8feb84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[2.7, 2.3, 3.3, 2.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9ae23",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "As we are dealing with lots of data and that data is in raw form, before inputting that data to machine learning algorithms, we need to convert it into meaningful data. This process is called **preprocessing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192339ce",
   "metadata": {},
   "source": [
    "#### Binarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f799b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d556c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input_data = np.array(\n",
    "    [\n",
    "        [2.1, -1.9, 5.5],\n",
    "        [-1.5, 2.4, 3.5],\n",
    "        [0.5, -7.9, 5.6],\n",
    "        [5.9, 2.3, -5.8],\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_binarized = preprocessing.Binarizer(threshold=0.5).transform(Input_data)\n",
    "data_binarized # NOTE threshold value = 0.5 and that is why, all the values above 0.5 would be converted to 1, and all the values below 0.5 would be converted to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af262cd",
   "metadata": {},
   "source": [
    "#### Mean Removal\n",
    "This technique is used to eliminate the mean from feature vector so that every feature centered on zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75752ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [ 1.75  -1.275  2.2  ]\n",
      "standard deviation:  [2.71431391 4.20022321 4.69414529]\n",
      "mean removed:  [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "std dev. removed:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \", Input_data.mean(axis=0))\n",
    "print(\"standard deviation: \", Input_data.std(axis=0))\n",
    "\n",
    "data_scaled = preprocessing.scale(Input_data)\n",
    "# Centers each feature around 0 (mean becomes 0), Normalizes standard deviation to 1\n",
    "\n",
    "print(\"mean removed: \", data_scaled.mean(axis=0))\n",
    "print(\"std dev. removed: \", data_scaled.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e2dfa",
   "metadata": {},
   "source": [
    "#### Scaling  \n",
    "We use this preprocessing technique for scaling the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e3c819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data: \n",
      " [[ 2.1 -1.9  5.5]\n",
      " [-1.5  2.4  3.5]\n",
      " [ 0.5 -7.9  5.6]\n",
      " [ 5.9  2.3 -5.8]]\n",
      "\n",
      "MinMax scaled data:\n",
      " [[0.48648649 0.58252427 0.99122807]\n",
      " [0.         1.         0.81578947]\n",
      " [0.27027027 0.         1.        ]\n",
      " [1.         0.99029126 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "minmax_data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_data_scaled = minmax_data_scaler.fit_transform(Input_data)\n",
    "# X_scaled = (X - X_min) / (X_max - X_min)\n",
    "print(\"Input Data: \\n\",Input_data)\n",
    "print(\"\\nMinMax scaled data:\\n\", minmax_data_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
